services:
    faster_whisper:
        build:
            context: https://github.com/DCC-BS/bentoml-faster-whisper.git
            dockerfile: ./Dockerfile
            platforms:
                - linux/amd64
                - linux/arm64
        ports:
            - "50001:50001"
        environment:
            - HF_AUTH_TOKEN=${HF_AUTH_TOKEN}
        volumes:
            - hugging_face_cache:/root/.cache/huggingface
        deploy:
            resources:
                reservations:
                    memory: 16g
                    devices:
                        - driver: nvidia
                          device_ids: ["1"]
                          capabilities: [gpu]
    llm:
        image: vllm/vllm-openai:v0.11.2
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          device_ids: ["0"]
                          capabilities: [gpu]
        volumes:
            - "${HOME}/.cache/huggingface:/root/.cache/huggingface"
        environment:
            - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
        ports:
            - "8001:8000"
        ipc: host
        command:
            - "--port"
            - "8000"
            - "--model"
            - "Qwen/Qwen3-32B-AWQ"
            - "--max-model-len"
            - "10000"
            - "--max-num-seqs"
            - "1"
            - "--kv-cache-dtype"
            - "fp8"
            - "--reasoning-parser"
            - "qwen3"
            - "--enable-auto-tool-choice"
            - "--tool-call-parser"
            - "hermes"
            - "--gpu-memory-utilization"
            - "0.95"
            - "--tensor-parallel-size"
            - "1"
            - "--uvicorn-log-level"
            - "warning"
            - "--disable-log-requests"
    transcribo-backend:
        build:
            context: https://github.com/DCC-BS/transcribo-backend.git
            dockerfile: ./Dockerfile
        depends_on:
            - faster_whisper
        ports:
            - "8000:8000"
        environment:
            - WHISPER_API=http://faster_whisper:50001/v1
    transcribo-frontend:
        build:
            context: .
            dockerfile: ./Dockerfile
        depends_on:
            - transcribo-backend
        ports:
            - "3000:3000"
        profiles:
            - frontend
volumes:
    hugging_face_cache:
